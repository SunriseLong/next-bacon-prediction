{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6gAORWqhy9aR",
    "outputId": "7d9be79c-088f-42ef-dc9f-9f741def7248"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "6I62iTr1zKwJ",
    "outputId": "5f6719af-36ff-4745-8a0f-4ba3be6c6963"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://raw.githubusercontent.com/harrymqz/data_mining//master/data/NONFICTION/bacon-essays-92.txt\n",
      "303104/296995 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "bacon_path = keras.utils.get_file(\n",
    "    \"bacon-essays-92.txt\",\n",
    "    origin= \"https://raw.githubusercontent.com/harrymqz/data_mining//master/data/NONFICTION/bacon-essays-92.txt\")\n",
    "\n",
    "bacon = open(bacon_path).read().lower()\n",
    "\n",
    "bacon_clean = bacon.replace(\"\\n\", \" \").replace(',', '')\n",
    "\n",
    "bacon_tok = re.findall(r\"[\\w']+|[.,!?;]\", bacon_clean[155:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fisjT8HazTHa"
   },
   "outputs": [],
   "source": [
    "# create dictionary of word counts\n",
    "word_dict = {}\n",
    "for w in bacon_tok:\n",
    "  if w in word_dict:\n",
    "    word_dict[w] += 1\n",
    "  else:\n",
    "    word_dict[w] = 1\n",
    "\n",
    "word_freq = sorted([(freq, w) for w, freq in word_dict.items()], reverse=True)\n",
    "words = [w[1] for w in word_freq]\n",
    "\n",
    "word_freq = word_freq[:1999]\n",
    "index2word = {i: w[1] for i, w in enumerate(word_freq, 0)}\n",
    "index2word[1999] = '<UNK>'\n",
    "word2index = {w: i for i, w in index2word.items()}\n",
    "\n",
    "# window over corpus and randomly sample phrases of 5-15 words\n",
    "index = 0\n",
    "X = []\n",
    "y = []\n",
    "for i in range(1, 2000):\n",
    "  rv = np.random.randint(5,15)\n",
    "  X.append(' '.join(bacon_tok[index: index+rv]))\n",
    "  y.append(bacon_tok[index+rv])\n",
    "  index+=(rv+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "uyHHdK3G35DX",
    "outputId": "df4fec6c-cac7-4460-f60a-533076bee95b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       ...,\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain = np.array(X, dtype=object)[:, np.newaxis]\n",
    "\n",
    "# OHE target\n",
    "ytrain = list(map(lambda w: word2index[w] if w in word2index else 1999, y))\n",
    "ytrain = np.eye(2000)[ytrain]\n",
    "ytrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uKLFvehT6VmH"
   },
   "outputs": [],
   "source": [
    "# ensure version 5. prev models are not keras serialization enabled\n",
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/5\"\n",
    "\n",
    "embed = hub.KerasLayer(module_url, output_shape=(512,), input_shape=[], dtype=tf.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "Lw_Ejunv6kE1",
    "outputId": "7ef301ba-5d57-4fbb-fd3f-184b55ea1e22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer (KerasLayer)     (None, 512)               147354880 \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2500)              1282500   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2000)              5002000   \n",
      "=================================================================\n",
      "Total params: 153,639,380\n",
      "Trainable params: 6,284,500\n",
      "Non-trainable params: 147,354,880\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(embed)\n",
    "model.add(tf.keras.layers.Dense(2500, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(2000, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vfOr4_Ml7Uhj"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(xtrain, ytrain, epochs=20, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "huNeQJas7XZ_"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "model_path = \"/tmp/model_artifacts\"\n",
    "model.save(model_path)\n",
    "files.download(\"/tmp/model_artifacts/saved_model.pb\") \n",
    "\n",
    "# save word index to word mapping for inference\n",
    "with open('/api/index2word.json', 'w') as fp:\n",
    "    json.dump(index2word, fp)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "use-demo.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}